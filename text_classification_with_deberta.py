# -*- coding: utf-8 -*-
"""Text Classification with DeBERTa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQlzX1riyaEd-r1zBm4PkL1gpBA_R5q5
"""

!pip install transformers datasets scikit-learn pandas openpyxl

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import DataCollatorWithPadding , TrainingArguments , Trainer
from datasets import Dataset
import torch
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import gradio as gr
import tempfile
import os

# Load the labeled data
df_labeled = pd.read_excel("DataLabel.xlsx")

# Clean and strip labels
df_labeled["label"] = df_labeled["label"].str.strip().str.title()

label_map = {"Complaint": 0, "Inquiry": 1, "Suggestion": 2, "Thank You": 3}
df_labeled["label"] = df_labeled["label"].map(label_map)
df_labeled.dropna(subset=["label"], inplace=True)

# Convert to integer
df_labeled["label"] = df_labeled["label"].astype(int)

df_labeled.dropna(subset=["text", "label"], inplace=True)
df_labeled = df_labeled[["text", "label"]]
df_labeled.head()
print(df_labeled['label'].unique())

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")
dataset = Dataset.from_pandas(df_labeled)

# Tokenize
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Train/test split
train_test = tokenized_dataset.train_test_split(test_size=0.2)
train_dataset = train_test["train"]
eval_dataset = train_test["test"]
import os
os.environ["WANDB_DISABLED"] = "true"

model = AutoModelForSequenceClassification.from_pretrained("microsoft/deberta-v3-base", num_labels=4)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=18,
    logging_strategy="steps",
    logging_steps=50,
    per_device_train_batch_size=6,
    per_device_eval_batch_size=6
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

preds = trainer.predict(eval_dataset)
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=1)

print(classification_report(y_true, y_pred, target_names=label_map.keys()))

os.environ["WANDB_DISABLED"] = "true"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Load and clean the unlabeled data
df_unlabeled = pd.read_excel("DataUnlabel.xlsx")
df_unlabeled.dropna(subset=["text"], inplace=True)
texts = list(df_unlabeled["text"])
batch_size = 32

all_predictions = []
all_confidences = []

model.eval()

for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]
    encodings = tokenizer(batch_texts, truncation=True, padding=True, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model(**encodings)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)
        confidence, predictions = torch.max(probs, dim=1)

    all_predictions.extend(predictions.cpu().numpy())
    all_confidences.extend(confidence.cpu().numpy())

    del encodings, outputs, logits, probs, predictions, confidence
    torch.cuda.empty_cache()

# Add predictions to DataFrame
df_unlabeled["predicted_label"] = all_predictions
df_unlabeled["confidence"] = all_confidences

# Filter high-confidence predictions
df_pseudo = df_unlabeled[df_unlabeled["confidence"] > 0.90].copy()
df_pseudo["label"] = df_pseudo["predicted_label"].astype(int)

print(f"âœ… Using {len(df_pseudo)} pseudo-labeled samples.")

# Combine with original labeled data
df_full = pd.concat([
    df_labeled[["text", "label"]],
    df_pseudo[["text", "label"]]
], ignore_index=True)

# Shuffle the dataset
df_full = df_full.sample(frac=1).reset_index(drop=True)

# Convert to HuggingFace Dataset
dataset_full = Dataset.from_pandas(df_full)

# Tokenize
tokenized_full = dataset_full.map(tokenize_function, batched=True)

# Train/test split again
train_test_full = tokenized_full.train_test_split(test_size=0.2)
train_dataset = train_test_full["train"]
eval_dataset = train_test_full["test"]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Re-initialize the Trainer with the new datasets
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

# Evaluate the retrained model
preds = trainer.predict(eval_dataset)
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=1)

print(classification_report(y_true, y_pred, target_names=label_map.keys()))

model.save_pretrained("Text_classifier_model")
tokenizer.save_pretrained("Text_classifier_model")

# Sample texts
texts = [
    "Iâ€™m really unhappy with the service I received.",
    "When will my package be delivered?",
    "It would be nice to have more payment options.",
    "Thank you for the quick support!",
    "Why was my account deactivated without notice?",
    "Could you help me change my subscription plan?",
    "I appreciate how responsive your team is.",
    "Maybe you could add dark mode to the app?",
    "Your service is the worst Iâ€™ve used so far.",
    "Thanks! Everything works perfectly now."
]

# True labels: 0 = Complaint, 1 = Inquiry, 2 = Suggestion, 3 = Thank You
true_labels = [0, 1, 2, 3, 0, 1, 3, 2, 0, 3]

label_map = {0: "Complaint", 1: "Inquiry", 2: "Suggestion", 3: "Thank You"}

# Inference function
def classify_text(text, model, tokenizer, device="cpu"):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred_class = torch.argmax(probs, dim=1).item()
    return pred_class

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Predict and print per sample
predicted_labels = []
print("ðŸ” Predictions:\n")
for i, text in enumerate(texts):
    pred = classify_text(text, model, tokenizer, device)
    predicted_labels.append(pred)
    print(f"{i+1}. Text: {text}")
    print(f"   â†’ Predicted: {label_map[pred]} | True: {label_map[true_labels[i]]}\n")

print("\nðŸ“Š Classification Report:\n")
print(classification_report(true_labels, predicted_labels, target_names=label_map.values()))

# Confusion Matrix
cm = confusion_matrix(true_labels, predicted_labels)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=label_map.values(), yticklabels=label_map.values())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (10 Real Examples)")
plt.tight_layout()
plt.show()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load saved model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained("Text_classifier_model").to(device)
tokenizer = AutoTokenizer.from_pretrained("Text_classifier_model")
label_map = {0: "Complaint", 1: "Inquiry", 2: "Suggestion", 3: "Thank You"}

# Prediction
def classify_text_gr(text):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs, dim=1).item()
        conf = torch.max(probs).item()
    return f"{label_map[pred]} (Confidence: {conf:.2f})"

# Gradio interface
gr.Interface(
    fn=classify_text_gr,
    inputs=gr.Textbox(lines=4, placeholder="Enter a message..."),
    outputs="text",
    title="Text Classifier",
    description="Classifies a message as Complaint, Inquiry, Suggestion, or Thank You.",
    examples=[
        ["I want to cancel my order, it never arrived."],
        ["Can I get more information about my subscription?"],
        ["You should offer a mobile app."],
        ["Thank you so much for your fast response!"]
    ]
).launch(share=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model + tokenizer once
model = AutoModelForSequenceClassification.from_pretrained("Text_classifier_model").to(device)
tokenizer = AutoTokenizer.from_pretrained("Text_classifier_model")
label_map = {0: "Complaint", 1: "Inquiry", 2: "Suggestion", 3: "Thank You"}

def classify_reviews(df):
    texts = df["text"].fillna("").tolist()
    batch_size = 32
    predictions = []
    confidences = []

    model.eval()
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            encodings = tokenizer(batch, truncation=True, padding=True, return_tensors="pt").to(device)
            outputs = model(**encodings)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            conf, preds = torch.max(probs, dim=1)
            predictions.extend(preds.cpu().numpy())
            confidences.extend(conf.cpu().numpy())

    df["predicted_label"] = [label_map[p] for p in predictions]
    df["confidence"] = confidences
    return df

def filter_and_save(file, category, min_confidence):
    # Read uploaded file
    ext = os.path.splitext(file.name)[1].lower()
    if ext == ".xlsx":
        df = pd.read_excel(file.name)
    elif ext == ".csv":
        df = pd.read_csv(file.name)
    else:
        raise ValueError("Unsupported file format! Please upload .csv or .xlsx file.")

    if "text" not in df.columns:
        raise ValueError("Uploaded file must contain a 'text' column.")

    df_classified = classify_reviews(df)
    filtered = df_classified[
        (df_classified["predicted_label"] == category) &
        (df_classified["confidence"] >= min_confidence)
    ]

    if filtered.empty:
        return None

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx")
    filtered.to_excel(temp_file.name, index=False)
    temp_file.close()

    return temp_file.name

# Gradio interface with confidence slider
iface = gr.Interface(
    fn=filter_and_save,
    inputs=[
        gr.File(file_types=[".csv", ".xlsx"], label="Upload review file"),
        gr.Dropdown(choices=list(label_map.values()), label="Select category to filter"),
        gr.Slider(0.0, 1.0, value=0.85, step=0.01, label="Minimum confidence")
    ],
    outputs=gr.File(label="Download filtered reviews"),
    title="Review Filter & Download",
    description="Upload a review file with a 'text' column. Select a category and confidence threshold to filter, and download the results as an Excel file."
)

iface.launch(share=True)